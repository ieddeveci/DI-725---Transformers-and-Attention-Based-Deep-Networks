{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdde2cb1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdde2cb1",
        "outputId": "5a3b7715-18ed-4f71-d96a-68ae3b4e9c8d"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebdd0fe",
      "metadata": {
        "id": "5ebdd0fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler\n",
        "from contextlib import nullcontext\n",
        "import time\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tiktoken\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8d9d9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "5b8d9d9c",
        "outputId": "41a9d5d0-d926-4439-dbb7-3eeef0f006f6"
      },
      "outputs": [],
      "source": [
        "# WandB login for monitoring training with parameters\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c42487",
      "metadata": {
        "id": "c3c42487"
      },
      "outputs": [],
      "source": [
        "# Model architecture\n",
        "# The original model architecture from the github repo was for text generation\n",
        "# Here, the architecture is modified for sentiment analysis. Added sentiment head for classification\n",
        "# Also, instead of loading a GPT2 model for text generation, GPT2 model for sequence classification is added for the GPT2 configurations\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True\n",
        "    num_classes: int = 3 # Added number of labels parameter\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        self.classifier = nn.Linear(config.n_embd, config.num_classes) # Modified for classification\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        # Classification head\n",
        "        x = x[:, -1, :]  # take the last token's representation\n",
        "        logits = self.classifier(x)  # project to classification logits\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        assert block_size <= self.config.block_size\n",
        "        self.config.block_size = block_size\n",
        "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
        "        for block in self.transformer.h:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type, override_args=None):\n",
        "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "        override_args = override_args or {}\n",
        "        assert all(k == 'dropout' for k in override_args)\n",
        "        from transformers import GPT2ForSequenceClassification # GPT2 for sequence classification is added instead of texthead due to key mismatch error\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        config_args = {\n",
        "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),\n",
        "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024),\n",
        "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280),\n",
        "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600),\n",
        "        }[model_type]\n",
        "\n",
        "        print(\"forcing vocab_size=50257, block_size=256, bias=True\")\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 256\n",
        "        config_args['bias'] = True\n",
        "        config_args['num_classes'] = 3  # Add this for classification\n",
        "\n",
        "        if 'dropout' in override_args:\n",
        "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "            config_args['dropout'] = override_args['dropout']\n",
        "\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "\n",
        "        # Load the pre-trained model, this is changed for sentiment analysis\n",
        "        num_labels = override_args.get(\"num_classes\", 3)\n",
        "        model_hf = GPT2ForSequenceClassification.from_pretrained(model_type, num_labels=num_labels)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        hf_to_our_map = {\n",
        "            'transformer.wte.': 'transformer.wte.',\n",
        "            'transformer.wpe.': 'transformer.wpe.',\n",
        "            'transformer.h.': 'transformer.h.',\n",
        "            'transformer.ln_f.': 'transformer.ln_f.',\n",
        "            'classifier.': 'classifier.'  # Map classification head\n",
        "        }\n",
        "\n",
        "        for k_hf in sd_hf.keys():\n",
        "            if k_hf.endswith('.attn.masked_bias') or k_hf.endswith('.attn.bias'):\n",
        "                continue\n",
        "\n",
        "            k_our = k_hf\n",
        "            for hf_prefix, our_prefix in hf_to_our_map.items():\n",
        "                if k_hf.startswith(hf_prefix):\n",
        "                    k_our = k_hf.replace(hf_prefix, our_prefix)\n",
        "                    break\n",
        "\n",
        "            if k_our in sd:\n",
        "                if any(k_our.endswith(w) for w in ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']):\n",
        "                    # Handle transposed weights\n",
        "                    if sd_hf[k_hf].shape[::-1] == sd[k_our].shape:\n",
        "                        with torch.no_grad():\n",
        "                            sd[k_our].copy_(sd_hf[k_hf].t())\n",
        "                else:\n",
        "                    # Regular weights\n",
        "                    if sd_hf[k_hf].shape == sd[k_our].shape:\n",
        "                        with torch.no_grad():\n",
        "                            sd[k_our].copy_(sd_hf[k_hf])\n",
        "\n",
        "        print(\"Loaded GPT2 for sequence classification\")\n",
        "        return model\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)\n",
        "        flops_promised = 312e12\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6209fe47",
      "metadata": {},
      "source": [
        "The following part is for data loading and tokenizing for NanoGPT, training, and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032fb6b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character level data preparation and tokenization for NanoGPT\n",
        "\n",
        "train_data_path = \"prep_train_final.csv\"\n",
        "val_data_path = \"prep_val_final.csv\"\n",
        "test_data_path = \"prep_test_final.csv\"\n",
        "batch_size = 12\n",
        "block_size = 1024\n",
        "\n",
        "def load_data(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    texts = df['conversation'].astype(str).tolist()\n",
        "    labels = df['customer_sentiment'].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_data(train_data_path)\n",
        "val_texts, val_labels = load_data(val_data_path)\n",
        "test_texts, test_labels = load_data(test_data_path)\n",
        "\n",
        "all_texts = train_texts + val_texts\n",
        "data = ' '.join(all_texts)\n",
        "chars = sorted(list(set(data)))\n",
        "vocab_size = len(chars)\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print(\"All unique characters:\")\n",
        "print(''.join(chars))\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "def encode(text):\n",
        "    return [stoi[c] for c in text if c in stoi]\n",
        "\n",
        "train_ids = [encode(text) for text in train_texts]\n",
        "val_ids = [encode(text) for text in val_texts]\n",
        "test_ids = [encode(text) for text in test_texts]\n",
        "\n",
        "def pad_or_truncate(data, block_size):\n",
        "    return [seq[:block_size] if len(seq) > block_size else seq + [0] * (block_size - len(seq)) for seq in data]\n",
        "\n",
        "train_ids = pad_or_truncate(train_ids, block_size)\n",
        "val_ids = pad_or_truncate(val_ids, block_size)\n",
        "test_ids = pad_or_truncate(test_ids, block_size)\n",
        "\n",
        "train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
        "val_ids = torch.tensor(val_ids, dtype=torch.long)\n",
        "test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
        "\n",
        "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "train_labels = torch.tensor([label_map[label.lower()] for label in train_labels], dtype=torch.long)\n",
        "val_labels = torch.tensor([label_map[label.lower()] for label in val_labels], dtype=torch.long)\n",
        "test_labels = torch.tensor([label_map[label.lower()] for label in test_labels], dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "val_dataset = TensorDataset(val_ids, val_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_ids)}\")\n",
        "print(f\"Number of validation samples: {len(val_ids)}\")\n",
        "print(f\"Number of test samples: {len(test_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbe7c42",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0dbe7c42",
        "outputId": "eadff998-d287-4542-f39e-2aebe37bb978"
      },
      "outputs": [],
      "source": [
        "# Training configuration and loop for training NanoGPT from scratch\n",
        "# train.py and prepare.py are unified, added early stopping\n",
        "\n",
        "init_from = 'scratch' # For training NanoGPT from scratch\n",
        "out_dir = 'out_sent_char'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "eval_interval = 100\n",
        "log_interval = 50\n",
        "eval_iters = 200\n",
        "max_iters = 2000\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "batch_size = batch_size\n",
        "block_size = block_size\n",
        "\n",
        "n_layer = 4\n",
        "n_head = 4\n",
        "n_embd = 256\n",
        "dropout = 0.0\n",
        "bias = True\n",
        "num_labels = 3\n",
        "\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "\n",
        "warmup_iters = 500\n",
        "lr_decay_iters = 600000\n",
        "min_lr = 6e-5\n",
        "\n",
        "backend = 'nccl'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "\n",
        "wandb_log = True\n",
        "wandb_project = 'sentiment_analysis_char'\n",
        "wandb_run_name = 'classification_from_scratch' + str(time.time())\n",
        "\n",
        "if wandb_log:\n",
        "    wandb.init(\n",
        "        project=wandb_project,\n",
        "        name=wandb_run_name,\n",
        "        config={\n",
        "            'init_from': init_from,\n",
        "            'n_layer': n_layer,\n",
        "            'n_head': n_head,\n",
        "            'n_embd': n_embd,\n",
        "            'dropout': dropout,\n",
        "            'num_labels': num_labels,\n",
        "            'max_iters': max_iters,\n",
        "            'eval_interval': eval_interval,\n",
        "            'log_interval': log_interval,\n",
        "            'eval_iters': eval_iters,\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'beta1': beta1,\n",
        "            'beta2': beta2,\n",
        "            'grad_clip': grad_clip,\n",
        "            'gradient_accumulation_steps': gradient_accumulation_steps,\n",
        "            'batch_size': batch_size,\n",
        "            'block_size': block_size,\n",
        "            'decay_lr': decay_lr,\n",
        "            'warmup_iters': warmup_iters,\n",
        "            'min_lr' : min_lr,\n",
        "        }\n",
        "    )\n",
        "\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "    seed_offset = ddp_rank\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# Load Data\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "\n",
        "def get_batch(split):\n",
        "    global train_iter, val_iter\n",
        "\n",
        "    iterator = train_iter if split == 'train' else val_iter\n",
        "\n",
        "    try:\n",
        "        x, y = next(iterator)\n",
        "    except StopIteration:\n",
        "        if split == 'train':\n",
        "            train_iter = iter(train_loader)\n",
        "            x, y = next(train_iter)\n",
        "        else:\n",
        "            val_iter = iter(val_loader)\n",
        "            x, y = next(val_iter)\n",
        "\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Initialize model\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
        "                  block_size=block_size, bias=bias, vocab_size=None,\n",
        "                  dropout=dropout)\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if vocab_size is None: # Vocab size is get from the initial dataset read\n",
        "        print(\"defaulting to vocab_size of GPT-2: 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = vocab_size if vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
        "        total_loss = 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            _, loss = model(xb, yb)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        out[split] = total_loss / len(loader)\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "val_loss_list = []\n",
        "early_stop_patience = 3\n",
        "best_val_loss = float('inf')\n",
        "iter_num = 0\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model = model.module if ddp else model\n",
        "running_mfu = -1.0\n",
        "X, Y = get_batch('train')\n",
        "\n",
        "def check_early_stopping(val_loss):\n",
        "    val_loss_list.append(val_loss)\n",
        "    if len(val_loss_list) > early_stop_patience:\n",
        "        val_loss_list.pop(0)\n",
        "        if all(val_loss >= prev for prev in val_loss_list):\n",
        "            print(f\"Early stopping: No improvement in validation loss for {early_stop_patience} evaluations.\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def save_checkpoint(val_loss, iter_num):\n",
        "    global best_val_loss\n",
        "    if val_loss < best_val_loss or always_save_checkpoint:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model_args': model_args,\n",
        "            'iter_num': iter_num,\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at iteration {iter_num}, validation loss: {val_loss:.4f}\")\n",
        "\n",
        "while iter_num <= max_iters:\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {val_loss:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100,\n",
        "            })\n",
        "\n",
        "        if check_early_stopping(val_loss):\n",
        "            break\n",
        "\n",
        "        save_checkpoint(val_loss, iter_num)\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if wandb_log:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40202f87",
      "metadata": {
        "id": "40202f87",
        "outputId": "ade009ad-08dd-4e5e-c34b-3a57ce32861a"
      },
      "outputs": [],
      "source": [
        "# Evaluation code for trained NanoGPT\n",
        "# Includes Accuracy, Class-wise accuracy (3 classes), F1-score, and confusion matrix\n",
        "\n",
        "checkpoint = torch.load('out_sent_char/ckpt.pt', map_location=device)\n",
        "state_dict = checkpoint['model']\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        logits, _ = model(inputs)\n",
        "        predicted_labels = logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "overall_accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"\\nOverall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "\n",
        "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "class_labels = ['positive', 'neutral', 'negative']\n",
        "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "print(\"\\nClass-wise Accuracy:\")\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    label_name = inv_label_map[i]\n",
        "    print(f\"  Class {i} ({label_name}): {acc * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(true_labels, predictions, target_names=[inv_label_map[i] for i in range(3)], zero_division=0)\n",
        "print(report)\n",
        "\n",
        "ordered_labels = ['negative', 'neutral', 'positive']\n",
        "ordered_indices = [label_map[label] for label in ordered_labels]\n",
        "reordered_conf_matrix = conf_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"{:<10}\".format(\"\"), end=\"\")\n",
        "for label in ordered_labels:\n",
        "    print(\"{:<10}\".format(label), end=\"\")\n",
        "print()\n",
        "for i, row_label in enumerate(ordered_labels):\n",
        "    print(\"{:<10}\".format(row_label), end=\"\")\n",
        "    for j in range(len(ordered_labels)):\n",
        "        print(\"{:<10}\".format(reordered_conf_matrix[i][j]), end=\"\")\n",
        "    print()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(reordered_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=ordered_labels, yticklabels=ordered_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0558ef",
      "metadata": {},
      "source": [
        "The following part is for data loading and tokenizing for GPT-2, fine-tuning, and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cdd8c76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cdd8c76",
        "outputId": "0adefdd6-6e73-43ee-8c21-907bccbd084b"
      },
      "outputs": [],
      "source": [
        "# Data preparation and tokenization for gpt2. Added gpt2 tokenizer instead of character level tokenization\n",
        "\n",
        "train_data_path = \"prep_train_final.csv\"\n",
        "val_data_path = \"prep_val_final.csv\"\n",
        "test_data_path = \"prep_test_final.csv\"\n",
        "\n",
        "block_size = 256\n",
        "batch_size = 24\n",
        "\n",
        "def load_data(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    texts = df['conversation'].astype(str).tolist()\n",
        "    labels = df['customer_sentiment'].tolist()\n",
        "    return texts, labels\n",
        "\n",
        "train_texts, train_labels = load_data(train_data_path)\n",
        "val_texts, val_labels = load_data(val_data_path)\n",
        "test_texts, test_labels = load_data(test_data_path)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def encode(text):\n",
        "    return enc.encode_ordinary(text)\n",
        "\n",
        "train_ids = [encode(text) for text in train_texts]\n",
        "val_ids = [encode(text) for text in val_texts]\n",
        "test_ids = [encode(text) for text in test_texts]\n",
        "\n",
        "def pad_or_truncate(data, block_size):\n",
        "    return [seq[:block_size] if len(seq) > block_size else seq + [enc.eot_token] * (block_size - len(seq)) for seq in data]\n",
        "\n",
        "train_ids = pad_or_truncate(train_ids, block_size)\n",
        "val_ids = pad_or_truncate(val_ids, block_size)\n",
        "test_ids = pad_or_truncate(test_ids, block_size)\n",
        "\n",
        "train_ids = torch.tensor(train_ids, dtype=torch.long)\n",
        "val_ids = torch.tensor(val_ids, dtype=torch.long)\n",
        "test_ids = torch.tensor(test_ids, dtype=torch.long)\n",
        "\n",
        "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "train_labels = torch.tensor([label_map[label.lower()] for label in train_labels], dtype=torch.long)\n",
        "val_labels = torch.tensor([label_map[label.lower()] for label in val_labels], dtype=torch.long)\n",
        "test_labels = torch.tensor([label_map[label.lower()] for label in test_labels], dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(train_ids, train_labels)\n",
        "val_dataset = TensorDataset(val_ids, val_labels)\n",
        "test_dataset = TensorDataset(test_ids, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_ids)}\")\n",
        "print(f\"Number of validation samples: {len(val_ids)}\")\n",
        "print(f\"Number of test samples: {len(test_ids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a876638c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "a876638c",
        "outputId": "280db5da-5763-4e24-d58f-21ed0ac440ea"
      },
      "outputs": [],
      "source": [
        "# Training configuration and loop for fine-tuning GPT2, the variables are also changed for fine-tuning\n",
        "# train.py and prepare.py are unified, added early stopping\n",
        "\n",
        "init_from = 'gpt2' # For fine-tuning GPT2\n",
        "out_dir = 'out_sent_fineTune'\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "eval_interval = 25\n",
        "log_interval = 1\n",
        "eval_iters = 25\n",
        "max_iters = 50\n",
        "\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "\n",
        "bias = False\n",
        "learning_rate = 1e-5\n",
        "weight_decay = 1e-1\n",
        "decay_lr = False\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "\n",
        "batch_size = batch_size\n",
        "block_size = block_size\n",
        "\n",
        "always_save_checkpoint = False\n",
        "\n",
        "dropout = 0.8\n",
        "gradient_accumulation_steps = 32\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "backend = 'nccl'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "\n",
        "wandb_log = True\n",
        "wandb_project = 'sentiment_analysis_gpt2'\n",
        "wandb_run_name = 'classification_fineTune' + str(time.time())\n",
        "\n",
        "if wandb_log:\n",
        "    wandb.init(\n",
        "        project=wandb_project,\n",
        "        name=wandb_run_name,\n",
        "        config={\n",
        "            'init_from': init_from,\n",
        "            'dropout': dropout,\n",
        "            'max_iters': max_iters,\n",
        "            'decay_lr': decay_lr,\n",
        "            'eval_interval': eval_interval,\n",
        "            'log_interval': log_interval,\n",
        "            'eval_iters': eval_iters,\n",
        "            'max_iters': max_iters,\n",
        "            'learning_rate': learning_rate,\n",
        "            'gradient_accumulation_steps': gradient_accumulation_steps,\n",
        "            'batch_size': batch_size,\n",
        "        }\n",
        "    )\n",
        "\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0\n",
        "    seed_offset = ddp_rank\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# Load Data\n",
        "train_iter = iter(train_loader)\n",
        "val_iter = iter(val_loader)\n",
        "\n",
        "def get_batch(split):\n",
        "    global train_iter, val_iter\n",
        "\n",
        "    iterator = train_iter if split == 'train' else val_iter\n",
        "\n",
        "    try:\n",
        "        x, y = next(iterator)\n",
        "    except StopIteration:\n",
        "        if split == 'train':\n",
        "            train_iter = iter(train_loader)\n",
        "            x, y = next(train_iter)\n",
        "        else:\n",
        "            val_iter = iter(val_loader)\n",
        "            x, y = next(val_iter)\n",
        "\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# Initialize model\n",
        "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
        "                  block_size=block_size, bias=bias, vocab_size=None,\n",
        "                  dropout=dropout)\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    print(\"Initializing a new model from scratch\")\n",
        "    if vocab_size is None: # Vocab size is get from the initial dataset read\n",
        "        print(\"defaulting to vocab_size of GPT-2: 50304 (50257 rounded up for efficiency)\")\n",
        "    model_args['vocab_size'] = vocab_size if vocab_size is not None else 50304\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "elif init_from == 'resume':\n",
        "    print(f\"Resuming training from {out_dir}\")\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    gptconf = GPTConfig(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k, v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "elif init_from.startswith('gpt2'):\n",
        "    print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
        "    override_args = dict(dropout=dropout)\n",
        "    model = GPT.from_pretrained(init_from, override_args)\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = getattr(model.config, k)\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size\n",
        "model.to(device)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
        "        total_loss = 0\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            _, loss = model(xb, yb)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        out[split] = total_loss / len(loader)\n",
        "\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "val_loss_list = []\n",
        "early_stop_patience = 3\n",
        "best_val_loss = float('inf')\n",
        "iter_num = 0\n",
        "local_iter_num = 0\n",
        "t0 = time.time()\n",
        "raw_model = model.module if ddp else model\n",
        "running_mfu = -1.0\n",
        "X, Y = get_batch('train')\n",
        "\n",
        "def check_early_stopping(val_loss):\n",
        "    val_loss_list.append(val_loss)\n",
        "    if len(val_loss_list) > early_stop_patience:\n",
        "        val_loss_list.pop(0)\n",
        "        if all(val_loss >= prev for prev in val_loss_list):\n",
        "            print(f\"Early stopping: No improvement in validation loss for {early_stop_patience} evaluations.\")\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def save_checkpoint(val_loss, iter_num):\n",
        "    global best_val_loss\n",
        "    if val_loss < best_val_loss or always_save_checkpoint:\n",
        "        best_val_loss = val_loss\n",
        "        checkpoint = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model_args': model_args,\n",
        "            'iter_num': iter_num,\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        print(f\"Checkpoint saved at iteration {iter_num}, validation loss: {val_loss:.4f}\")\n",
        "\n",
        "while iter_num <= max_iters:\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {val_loss:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100,\n",
        "            })\n",
        "\n",
        "        if check_early_stopping(val_loss):\n",
        "            break\n",
        "\n",
        "        save_checkpoint(val_loss, iter_num)\n",
        "\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        X, Y = get_batch('train')\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5:\n",
        "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if wandb_log:\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ba0050",
      "metadata": {
        "id": "39ba0050",
        "outputId": "e5adbf16-510f-4497-9f97-aca0d479f870"
      },
      "outputs": [],
      "source": [
        "# Evaluation code for fine-tuned GPT2\n",
        "# Includes Accuracy, Class-wise accuracy (3 classes), F1-score, and confusion matrix\n",
        "\n",
        "checkpoint = torch.load('out_sent_fineTune/ckpt.pt', map_location=device)\n",
        "state_dict = checkpoint['model']\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        logits, _ = model(inputs)\n",
        "        predicted_labels = logits.argmax(dim=-1).cpu().numpy()\n",
        "\n",
        "        predictions.extend(predicted_labels)\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "conf_matrix = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "overall_accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"\\nOverall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
        "\n",
        "class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "class_labels = ['positive', 'neutral', 'negative']\n",
        "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "print(\"\\nClass-wise Accuracy:\")\n",
        "for i, acc in enumerate(class_accuracy):\n",
        "    label_name = inv_label_map[i]\n",
        "    print(f\"  Class {i} ({label_name}): {acc * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "report = classification_report(true_labels, predictions, target_names=[inv_label_map[i] for i in range(3)], zero_division=0)\n",
        "print(report)\n",
        "\n",
        "ordered_indices = [label_map[label] for label in class_labels]\n",
        "reordered_conf_matrix = conf_matrix[np.ix_(ordered_indices, ordered_indices)]\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"{:<10}\".format(\"\"), end=\"\")\n",
        "for label in class_labels:\n",
        "    print(\"{:<10}\".format(label), end=\"\")\n",
        "print()\n",
        "for i, row_label in enumerate(class_labels):\n",
        "    print(\"{:<10}\".format(row_label), end=\"\")\n",
        "    for j in range(len(class_labels)):\n",
        "        print(\"{:<10}\".format(reordered_conf_matrix[i][j]), end=\"\")\n",
        "    print()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(reordered_conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19874fbb",
      "metadata": {
        "id": "19874fbb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

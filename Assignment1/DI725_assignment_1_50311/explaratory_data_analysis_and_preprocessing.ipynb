{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test data\n",
    "train_path = \"customer_service/main/train.csv\"\n",
    "test_path =  \"customer_service/main/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the train data\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the test data\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names and number of rows and columns\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column names and number of rows and columns\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the train data\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the test data\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in the train data\n",
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows in the test data\n",
    "df_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for statistics in the train data\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for statistics in the test data\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution in the training data\n",
    "sns.countplot(data=df_train, x='customer_sentiment')\n",
    "plt.title('Distribution of Customer Sentiment')\n",
    "plt.show()\n",
    "\n",
    "df_train['customer_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution in the test data\n",
    "sns.countplot(data=df_test, x='customer_sentiment')\n",
    "plt.title('Distribution of Customer Sentiment')\n",
    "plt.show()\n",
    "\n",
    "df_test['customer_sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the length of the conversations in the train data\n",
    "df_train['conversation_length'] = df_train['conversation'].apply(len)\n",
    "sns.histplot(df_train['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths (Character Count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the length of the conversations in the test data\n",
    "df_test['conversation_length'] = df_test['conversation'].apply(len)\n",
    "sns.histplot(df_test['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths (Character Count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of other categorical features with respect to sentiment\n",
    "categorical_columns = ['issue_area', 'product_category', 'issue_complexity', 'agent_experience_level']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df_train, x=col, hue='customer_sentiment')\n",
    "    plt.title(f'Distribution of {col} by Sentiment')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform chi-square test for each categorical column to see whether there are correlations between the categorical features and the customer_sentiment\n",
    "def chi_square_test(col):\n",
    "    contingency_table = pd.crosstab(df_train[col], df_train['customer_sentiment'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    return chi2, p\n",
    "\n",
    "chi2_results = {}\n",
    "for col in categorical_columns:\n",
    "    chi2_results[col] = chi_square_test(col)\n",
    "\n",
    "chi2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the results, there seems to be a significant relationship between issue_area and customer_sentiment. There is an almost significant relation between agent_experience_level and customer_sentiment since the p_value is so close to 0.05. There seems no significant relation between customer_sentiment and product_category and issue_complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the most frequent words across all conversations and their counts\n",
    "text = \" \".join(df_train['conversation'])\n",
    "word_counts = Counter(text.split())\n",
    "most_common_words = word_counts.most_common(15)\n",
    "print(\"Most 15 frequent words:\")\n",
    "for word, count in most_common_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the relevant columns for the sentiment-analysis task, splitting the train data into training and validation, and saving the new train-val-test data files.\n",
    "df_train_final = df_train[['conversation', 'customer_sentiment']]\n",
    "df_test_final = df_test[['conversation', 'customer_sentiment']]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df_train_final['conversation'], df_train_final['customer_sentiment'], \n",
    "    test_size=0.2, random_state=42, stratify=df_train['customer_sentiment'])\n",
    "\n",
    "df_train_final = pd.DataFrame({'conversation': X_train, 'customer_sentiment': y_train})\n",
    "df_val_final = pd.DataFrame({'conversation': X_val, 'customer_sentiment': y_val})\n",
    "\n",
    "df_train_final.to_csv(\"customer_service/conversationOnly/train_final.csv\", index=False)\n",
    "df_val_final.to_csv(\"customer_service/conversationOnly/val_final.csv\", index=False)\n",
    "df_test_final.to_csv(\"customer_service/conversationOnly/test_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final.loc[:, 'conversation_length'] = df_train_final['conversation'].apply(len)\n",
    "df_val_final.loc[:, 'conversation_length'] = df_val_final['conversation'].apply(len)\n",
    "df_test_final.loc[:, 'conversation_length'] = df_test_final['conversation'].apply(len)\n",
    "\n",
    "# Visualization of the length of the conversations in the new train data\n",
    "sns.histplot(df_train_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Train Data (Character Count)')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of the length of the conversations in the validation data\n",
    "sns.histplot(df_val_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Validation Data (Character Count)')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of the length of the conversations in the test data\n",
    "sns.histplot(df_test_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Test Data (Character Count)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data\n",
    "def order_number_pattern(sentence):\n",
    "    order_pattern = r'\\bbb\\d+\\b'\n",
    "    return re.search(order_pattern, sentence)\n",
    "\n",
    "def preprocess_conversation(text):\n",
    "    # Lowercase the entire text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove any text within square brackets and parentheses\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Split the conversation into lines\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    # Keep only the lines that start with \"customer:\"\n",
    "    customer_lines = [line for line in lines if line.strip().startswith(\"customer:\")]\n",
    "    \n",
    "    # Remove empty lines and trim each line\n",
    "    customer_lines = [line.strip() for line in customer_lines if line.strip()]\n",
    "    \n",
    "    # Delete \"customer:\" headings from the remaining lines\n",
    "    customer_lines = [line.replace(\"customer: \", \"\") for line in customer_lines]\n",
    "    \n",
    "    # Delete sentences that contain order numbers\n",
    "    customer_lines = [line for line in customer_lines if not order_number_pattern(line)]\n",
    "    \n",
    "    # Combine the processed lines back into a single string.\n",
    "    return \" \".join(customer_lines)\n",
    "\n",
    "# Example\n",
    "conversation = \"\"\"Agent: Thank you for contacting BrownBox customer support. My name is John. How can I assist you today?\n",
    "\n",
    "Customer: Hi John. I have been trying to order a refrigerator from your website, but it's not available for shipping to my location. Can you help me with this?\n",
    "\n",
    "Agent: I'm sorry to hear that, sir. May I have your location, please?\n",
    "\n",
    "Customer: Yes, I am in New York.\n",
    "\n",
    "Agent: Thank you, sir. I apologize for the inconvenience, but due to some logistic issues, we are unable to ship refrigerators to New York currently. However, we have other products available for shipping to your location. May I suggest some alternatives?\n",
    "\n",
    "Customer: No, I specifically want a refrigerator. This is unacceptable. Why can't you ship it to New York?\n",
    "\n",
    "Agent: I understand your frustration, sir. Unfortunately, we are experiencing some issues with our shipping partners, which is causing a delay in delivering certain products to some locations. We are working to resolve this as soon as possible.\n",
    "\n",
    "Customer: This is ridiculous. I need a refrigerator urgently. Can't you make an exception for me?\n",
    "\n",
    "Agent: I'm sorry, sir, but we are unable to make an exception in this case. However, I can suggest some local stores in your area where you may be able to purchase a refrigerator.\n",
    "\n",
    "Customer: I don't have time for that. This is a waste of my time. I want to speak to your manager.\n",
    "\n",
    "Agent: I apologize for the inconvenience, sir. I will transfer your call to my manager, who will be able to assist you better. Please hold the line.\n",
    "\n",
    "(Customer is on hold for a few minutes)\n",
    "\n",
    "Manager: Hi, this is Mark. How can I assist you today?\n",
    "\n",
    "Customer: Hi Mark. I am really disappointed with your service. I want to order a refrigerator, but your agent informed me that it's not available for shipping to my location.\n",
    "\n",
    "Manager: I'm sorry to hear that, sir. May I have your location, please?\n",
    "\n",
    "Customer: I am in New York.\n",
    "\n",
    "Manager: Yes, sir. I understand the issue. As my colleague informed you, we are experiencing some issues with our shipping partners, which is causing a delay in delivering certain products to some locations. However, I can assure you that we are working to resolve this as soon as possible.\n",
    "\n",
    "Customer: This is unacceptable. I need a refrigerator urgently.\n",
    "\n",
    "Manager: I understand your urgency, sir. However, as of now, we are unable to ship refrigerators to your location. I can suggest some local stores in your area where you may be able to purchase a refrigerator.\n",
    "\n",
    "Customer: I don't have time for that. This is a waste of my time. I am never going to order anything from your website again.\n",
    "\n",
    "Manager: I apologize for the inconvenience, sir. I understand your frustration, but please know that we are doing everything we can to resolve this issue. Is there anything else I can assist you with?\n",
    "\n",
    "Customer: No, that's all. Goodbye.\n",
    "\n",
    "Manager: I'm sorry to hear that, sir. Thank you for your time. Goodbye.,negative\"\"\"\n",
    "    \n",
    "processed_text = preprocess_conversation(conversation)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing approach focuses on cleaning and structuring the text to highlight relevant customer feedback while removing noise. It standardizes the content by lowercasing the text, which is a standard practice in NLP to reduce variations due to case sensitivity. The filtration ensures that the analysis remains focused on the customer's sentiments, which are the primary target for sentiment analysis. Removing text within square brackets and parentheses eliminates non-conversational elements (for example, agent's side interactions) that could distract from the emotional tone. Excluding order numbers removes irrelevant details that do not contribute to sentiment, such as logistical information. In conclusion, this preprocessing method isolates the customerâ€™s expressed emotions, making the sentiment signal clearer and enhancing the accuracy of the sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying preprocessing to new train-val-test files and saving them to another location\n",
    "\n",
    "df_train_final = pd.read_csv(\"customer_service/conversationOnly/train_final.csv\")\n",
    "df_val_final = pd.read_csv(\"customer_service/conversationOnly/val_final.csv\")\n",
    "df_test_final = pd.read_csv(\"customer_service/conversationOnly/test_final.csv\")\n",
    "\n",
    "df_train_final['conversation'] = df_train_final['conversation'].apply(preprocess_conversation)\n",
    "df_val_final['conversation'] = df_val_final['conversation'].apply(preprocess_conversation)\n",
    "df_test_final['conversation'] = df_test_final['conversation'].apply(preprocess_conversation)\n",
    "\n",
    "df_train_final.to_csv(\"customer_service/preprocessed/prep_train_final.csv\", index=False)\n",
    "df_val_final.to_csv(\"customer_service/preprocessed/prep_val_final.csv\", index=False)\n",
    "df_test_final.to_csv(\"customer_service/preprocessed/prep_test_final.csv\", index=False)\n",
    "\n",
    "df_val_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_final.loc[:, 'conversation_length'] = df_train_final['conversation'].apply(len)\n",
    "df_val_final.loc[:, 'conversation_length'] = df_val_final['conversation'].apply(len)\n",
    "df_test_final.loc[:, 'conversation_length'] = df_test_final['conversation'].apply(len)\n",
    "\n",
    "# Visualization of the length of the conversations in the preprocessed train data\n",
    "sns.histplot(df_train_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Train Data (Character Count)')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of the length of the conversations in the preprocessed validation data\n",
    "sns.histplot(df_val_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Validation Data (Character Count)')\n",
    "plt.show()\n",
    "\n",
    "# Visualization of the length of the conversations in the preprocessed test data\n",
    "sns.histplot(df_test_final['conversation_length'], bins=30, kde=True)\n",
    "plt.title('Distribution of Conversation Lengths in Test Data (Character Count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach seems to reduce most of the characters in all data files in the sense that it will significantly shorten the text sequences while preserving essential customer sentiment information. This reduction is particularly necessary for NanoGPT training and GPT-2 fine-tuning because these models have token length constraints, and excessive text could lead to inefficient training or truncation. By filtering out irrelevant content, such as agent responses, headings, and order numbers, the dataset becomes more focused on customer expressions of sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

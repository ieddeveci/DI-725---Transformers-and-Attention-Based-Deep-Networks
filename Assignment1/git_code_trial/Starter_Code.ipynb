{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e259e7b-d05b-41b8-b596-6ecdd4866c60",
   "metadata": {},
   "source": [
    "# DI 725: Transformers and Attention-Based Deep Networks\n",
    "\n",
    "## An Assignment for Implementing Transformers in PyTorch\n",
    "\n",
    "The purpose of this notebook is to guide you through the usage of sample code.\n",
    "\n",
    "This notebook follows the baseline prepared by Andrej Karpathy, with a custom dataset (Don-Quixote by Cervantes). This version of the code, called [nanoGPT](https://github.com/karpathy/nanoGPT), is a revisit to his famous [minGPT](https://github.com/karpathy/minGPT).\n",
    "### Author:\n",
    "* Ümit Mert Çağlar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715be989-8426-4406-bd8f-2bcf0e003f09",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "Install requirements for your environment, comment out for later uses.\n",
    "\n",
    "Dependencies:\n",
    "\n",
    "- [pytorch](https://pytorch.org)\n",
    "- [numpy](https://numpy.org/install/)\n",
    "-  `transformers` for huggingface transformers (to load GPT-2 checkpoints)\n",
    "-  `datasets` for huggingface datasets (to download + preprocess datasets)\n",
    "-  `tiktoken` for OpenAI's fast BPE code\n",
    "-  `wandb` for optional logging\n",
    "-  `tqdm` for progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e72d12-9aa6-456f-ae34-2c52aaeee7c3",
   "metadata": {},
   "source": [
    "The fastest way to get started to transformers, apart from following the labs of DI725, is to use a small model and dataset. For this purpose, we will start with training a character-level GPT on the Don-Quixote by Cervantes. The code will download a single file (2MB) and apply some transformations. Examine the code [prepare.py](data/don_char/prepare.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa2cade-4742-4b44-bcb2-0ae72c9571ad",
   "metadata": {},
   "source": [
    "## Quick Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6869a547-0ebb-4be6-9b25-f158db64407e",
   "metadata": {},
   "source": [
    "Use the following to prepare the don-quixote novel treated in character level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a08a93-5556-4cd9-ad2d-cc58d0363d52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 2,361,834\n",
      "all the unique characters: \n",
      " !#$%&()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÁÆÑÚàáæèéëíñóùŒœ—‘’“”•™\n",
      "vocab size: 105\n",
      "train has 2,125,650 tokens\n",
      "val has 236,184 tokens\n"
     ]
    }
   ],
   "source": [
    "!python data/don_char/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a34276-d844-435d-9e16-12f567969d5f",
   "metadata": {},
   "source": [
    "This creates a `train.bin` and `val.bin` in that data directory. Now it is time to train our own GPT. The size of the GPT model depends on the computational resources. It is advised to have a GPU for heavy works, and to train lightweight and evaluate and infer models with a CPU.\n",
    "\n",
    "Small scale GPT with the settings provided in the [config/train_don_char.py](config/train_don_char.py) config file will be trained with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad3097d-65d8-4a72-a8d0-b5fa463b49c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_don_char.py --device=cuda --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a06da9f-a5a1-4621-806c-adad9b2d98d5",
   "metadata": {},
   "source": [
    "We are training a small scaled GPT with a context size of up to 256 characters, 384 feature channels, 6 layers of transformer with 6 attention heads. On one GTX 3070 GPU this training run takes about 10 minutes and the best validation loss is 1.1620. Based on the configuration, the model checkpoints are being written into the `--out_dir` directory `out-don-char`. So once the training finishes we can sample from the best model by pointing the sampling script at this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d3380c5-976e-4dbb-b5dc-08ba56f0d93c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don-char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43535f2-8e3f-4823-955d-a30c9ed9e0eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "This generates a few samples, for example:\n",
    "\n",
    "```\n",
    "“I grant all that,” said the governor; “it’s not in a low voice\n",
    "\n",
    "but not yet forget that there’s none of it the poor in the world; I’ll\n",
    "\n",
    "like to take special to have been no one to write out the stone of\n",
    "\n",
    "patience to the village.”\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61966bc-558b-4964-8575-1046d0aa6a91",
   "metadata": {},
   "source": [
    "It is pretty nice to have a GPT in a few minutes of character level training! Better results can be achieved possibly by hyperparameter tuning and finetuning (transfer learning) from a pre-trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b323fd-20ba-4e6b-8a5a-e40b9fdd0bca",
   "metadata": {},
   "source": [
    "## Quick start with less resources\n",
    "\n",
    "If we are [low on resources](https://www.youtube.com/watch?v=rcXzn6xXdIc), we can use a simpler version of the training, first we need to set compile to false, this is also a must for Windows OS for now. We also set the device to CPU. The model that is trained in 10 minutes for a starter grade GPU, will be trained in a much longer time, so we can also decrease the dimensions of our model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef08093-984e-4fae-a1f9-fdf6bf4b1bd5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding config with config/train_don_char.py:\n",
      "# train a miniature character-level shakespeare model\n",
      "# good for debugging and playing on macbooks and such\n",
      "\n",
      "out_dir = 'out-don-char'\n",
      "eval_interval = 250  # keep frequent because we'll overfit\n",
      "eval_iters = 200\n",
      "log_interval = 50  # don't print too often\n",
      "\n",
      "# we expect to overfit on this small dataset, so only save when val improves\n",
      "always_save_checkpoint = False\n",
      "\n",
      "wandb_log = False  # override via command line if you like\n",
      "wandb_project = 'don-char'\n",
      "wandb_run_name = 'mini-gpt'\n",
      "\n",
      "dataset = 'don_char'\n",
      "gradient_accumulation_steps = 1\n",
      "batch_size = 64\n",
      "block_size = 256  # context of up to 256 previous characters\n",
      "\n",
      "# baby GPT model :)\n",
      "n_layer = 6\n",
      "n_head = 6\n",
      "n_embd = 384\n",
      "dropout = 0.2\n",
      "\n",
      "learning_rate = 1e-3  # with baby networks can afford to go a bit higher\n",
      "max_iters = 2000\n",
      "lr_decay_iters = 2000  # make equal to max_iters usually\n",
      "min_lr = 1e-4  # learning_rate / 10 usually\n",
      "beta2 = 0.99  # make a bit bigger because number of tokens per iter is small\n",
      "\n",
      "warmup_iters = 100  # not super necessary potentially\n",
      "\n",
      "# on macbook also add\n",
      "# device = 'cpu'  # run on cpu only\n",
      "# compile = False # do not torch compile the model\n",
      "\n",
      "Overriding: device = cuda\n",
      "Overriding: wandb_log = True\n",
      "Overriding: wandb_project = gpt2Train\n",
      "Overriding: wandb_run_name = trial1\n",
      "Overriding: out_dir = out-don-small-char\n",
      "Overriding: compile = False\n",
      "Overriding: eval_iters = 20\n",
      "Overriding: log_interval = 50\n",
      "Overriding: block_size = 64\n",
      "Overriding: batch_size = 12\n",
      "Overriding: n_layer = 6\n",
      "Overriding: n_head = 4\n",
      "Overriding: n_embd = 128\n",
      "Overriding: max_iters = 2000\n",
      "Overriding: lr_decay_iters = 1000\n",
      "Overriding: dropout = 0.2\n",
      "tokens per iteration will be: 768\n",
      "found vocab_size = 105 (inside data\\don_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "number of parameters: 1.19M\n",
      "num decayed parameter tensors: 26, with 1,201,280 parameters\n",
      "num non-decayed parameter tensors: 13, with 1,664 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 4.6505, val loss 4.6535\n",
      "iter 0: loss 4.6687, time 3808.46ms, mfu -100.00%\n",
      "iter 50: loss 2.8903, time 30.93ms, mfu 0.06%\n",
      "iter 100: loss 2.7203, time 80.24ms, mfu 0.06%\n",
      "iter 150: loss 2.4839, time 120.17ms, mfu 0.05%\n",
      "iter 200: loss 2.5139, time 66.44ms, mfu 0.05%\n",
      "step 250: train loss 2.4391, val loss 2.4633\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 250: loss 2.4446, time 3955.93ms, mfu 0.05%\n",
      "iter 300: loss 2.3015, time 81.73ms, mfu 0.04%\n",
      "iter 350: loss 2.4462, time 57.37ms, mfu 0.04%\n",
      "iter 400: loss 2.3701, time 38.80ms, mfu 0.04%\n",
      "iter 450: loss 2.3135, time 44.29ms, mfu 0.04%\n",
      "step 500: train loss 2.3109, val loss 2.3600\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 500: loss 2.3966, time 3623.14ms, mfu 0.04%\n",
      "iter 550: loss 2.3987, time 89.55ms, mfu 0.04%\n",
      "iter 600: loss 2.2267, time 54.69ms, mfu 0.04%\n",
      "iter 650: loss 2.2494, time 93.19ms, mfu 0.04%\n",
      "iter 700: loss 2.3603, time 82.36ms, mfu 0.03%\n",
      "step 750: train loss 2.2070, val loss 2.2749\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 750: loss 2.3601, time 4491.08ms, mfu 0.03%\n",
      "iter 800: loss 2.2870, time 87.47ms, mfu 0.03%\n",
      "iter 850: loss 2.3460, time 72.81ms, mfu 0.03%\n",
      "iter 900: loss 2.1668, time 88.49ms, mfu 0.03%\n",
      "iter 950: loss 2.3209, time 54.97ms, mfu 0.03%\n",
      "step 1000: train loss 2.1488, val loss 2.1989\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 1000: loss 2.2727, time 3526.63ms, mfu 0.03%\n",
      "iter 1050: loss 2.2311, time 76.53ms, mfu 0.03%\n",
      "iter 1100: loss 2.1837, time 82.10ms, mfu 0.03%\n",
      "iter 1150: loss 2.3151, time 66.44ms, mfu 0.03%\n",
      "iter 1200: loss 2.2202, time 65.41ms, mfu 0.03%\n",
      "step 1250: train loss 2.1043, val loss 2.1582\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 1250: loss 2.2698, time 4044.79ms, mfu 0.02%\n",
      "iter 1300: loss 2.2293, time 82.98ms, mfu 0.02%\n",
      "iter 1350: loss 2.2840, time 64.66ms, mfu 0.02%\n",
      "iter 1400: loss 2.2197, time 80.66ms, mfu 0.02%\n",
      "iter 1450: loss 2.2013, time 64.76ms, mfu 0.02%\n",
      "step 1500: train loss 2.1087, val loss 2.1411\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 1500: loss 2.1786, time 3699.60ms, mfu 0.02%\n",
      "iter 1550: loss 2.1537, time 91.66ms, mfu 0.02%\n",
      "iter 1600: loss 2.1801, time 101.10ms, mfu 0.02%\n",
      "iter 1650: loss 2.1924, time 64.80ms, mfu 0.02%\n",
      "iter 1700: loss 2.2137, time 106.49ms, mfu 0.02%\n",
      "step 1750: train loss 2.0858, val loss 2.1410\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 1750: loss 2.1039, time 3941.84ms, mfu 0.02%\n",
      "iter 1800: loss 2.0834, time 84.53ms, mfu 0.02%\n",
      "iter 1850: loss 2.1570, time 63.96ms, mfu 0.02%\n",
      "iter 1900: loss 2.2281, time 65.68ms, mfu 0.02%\n",
      "iter 1950: loss 2.1517, time 42.05ms, mfu 0.02%\n",
      "step 2000: train loss 2.0542, val loss 2.1197\n",
      "saving checkpoint to out-don-small-char\n",
      "iter 2000: loss 2.0827, time 3844.30ms, mfu 0.02%\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mtrial1\u001b[0m at: \u001b[34mhttps://wandb.ai/iedibrahimethemdeveci-metu-middle-east-technical-university/gpt2Train/runs/st80a6v1\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb\\run-20250403_211417-st80a6v1\\logs\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: iedibrahimethemdeveci (iedibrahimethemdeveci-metu-middle-east-technical-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "wandb: creating run\n",
      "wandb: Tracking run with wandb version 0.19.8\n",
      "wandb: Run data is saved locally in c:\\Users\\iedev\\Desktop\\SPRING 2025\\DI 725 - Transformers\\Assignments\\Assignment_1\\Trial_Generation\\DI725-main\\assignment_1\\wandb\\run-20250403_211417-st80a6v1\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run trial1\n",
      "wandb:  View project at https://wandb.ai/iedibrahimethemdeveci-metu-middle-east-technical-university/gpt2Train\n",
      "wandb:  View run at https://wandb.ai/iedibrahimethemdeveci-metu-middle-east-technical-university/gpt2Train/runs/st80a6v1\n"
     ]
    }
   ],
   "source": [
    "!python train.py config/train_don_char.py --device=cuda --wandb_log=True --wandb_project=\"gpt2Train\" --wandb_run_name=\"trial1\" --out_dir=\"out-don-small-char\" --compile=False --eval_iters=20 --log_interval=50 --block_size=64 --batch_size=12 --n_layer=6 --n_head=4 --n_embd=128 --max_iters=2000 --lr_decay_iters=1000 --dropout=0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2d4052-9674-49d9-91a4-c79c887e4c93",
   "metadata": {
    "tags": []
   },
   "source": [
    "*Here, since we are running on CPU instead of GPU we must set both `--device=cpu` and also turn off PyTorch 2.0 compile with `--compile=False`. Then when we evaluate we get a bit more noisy but faster estimate (`--eval_iters=20`, down from 200), our context size is only 64 characters instead of 256, and the batch size only 12 examples per iteration, not 64. We'll also use a much smaller Transformer (4 layers, 4 heads, 128 embedding size), and decrease the number of iterations to 2000 (and correspondingly usually decay the learning rate to around max_iters with `--lr_decay_iters`). Because our network is so small we also ease down on regularization (`--dropout=0.0`). This still runs in about ~5 minutes, but gets us a loss of only 1.88 and therefore also worse samples, but it's still good fun:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b15c137-c810-495b-8962-573f7c4a7d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: out_dir = out-don-small-char\n",
      "Overriding: device = cpu\n",
      "number of parameters: 1.19M\n",
      "Loading meta from data\\don_char\\meta.pkl...\n",
      "\n",
      "and of and ishe am lor ainter, if grutin the be tos the lis aprot,\n",
      "\n",
      "ors this rert an dor had mee of in ghim othe as the baist, fif ash or rerckid merel\n",
      "\n",
      "the ome a the ghe thers hides fo dore gresis lote, neasent of pencabes,\n",
      "\n",
      "and thas the bantanennd to an thas I ma beeecces thald un or tho he gucave\n",
      "\n",
      "me thid les of to or o matightad the cay by eend\n",
      "\n",
      "d chis yof hey, bres iw win un hochor the he I thas sanim sacke to the cow\n",
      "\n",
      "yow, and I o iSancht ant in not lase thase Go, and eerd and le wathir\n",
      "\n",
      "b\n",
      "---------------\n",
      "\n",
      "ma, ind forsevemed hin the trameat dit wan the ouncke nelercene abat the of the ece\n",
      "\n",
      "cof winth the would somangh thas a slaid nde heat the o a his dand\n",
      "\n",
      "herghen und tho ward noce a a which me wich ithearl here be on seved the co thar\n",
      "\n",
      "whe as wa him aredelllt at thime bute but wa\n",
      "\n",
      "o an no f thist iy thare hee wharm the sa  of theld gooide thage\n",
      "\n",
      "hive denove’s, that and sourn vend thive nout wand whin co, gre\n",
      "\n",
      "and dnonjongs he to ve by gureriting of be and ast his linghtepp, hend I\n",
      "\n",
      "wang fut of ch\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "Fomes he grid Quixot; Cand o hy sand fo whe’s all dend toorser mind\n",
      "\n",
      "od r17e I Sasthe thil this is eand co of sintate hars it to lat as iss\n",
      "\n",
      "wasthou the meant is sheeplle and Ca or ind the Sa and he yo mough to se\n",
      "\n",
      "of hegory oe of-tanguteed are the is the he af le hor hol buspellicttaviling,\n",
      "\n",
      "re ame whaitht was rea owancers bas ind on knouge tho be me and ry buckedersie, yo\n",
      "\n",
      "eay thar and e asu inte reteet afl or theys bece win is and\n",
      "\n",
      "whend to the plom. Quim\n",
      "\n",
      "\n",
      "\n",
      "and sas pilon, nochor heam foust\n",
      "---------------\n",
      "\n",
      "lisuriceg\n",
      "\n",
      "en uredme thay what mo wichtr sso to the e dut co the wagit of byo the wa in deand shas\n",
      "\n",
      "his at bre theruen her bbnomesenou the dothe sus asty to reit yo ar his thi\n",
      "\n",
      "the yee as in bellon to he im becitt ante et the; foo cof and dre ethin of the\n",
      "\n",
      "sad lo with that core itant whalld to he bin you ing mend tho beandng the wethe\n",
      "\n",
      "ut them, I than her hour to thim ato the as hal and the hin hof seme wherm\n",
      "\n",
      "word pen inicen, and and hins wa mee us set abe urn bon and thalf\n",
      "\n",
      "prove he is thad wa\n",
      "---------------\n",
      "\n",
      "Quixoton Sanjothis he fo hise te the imle of the hime ith pre knot\n",
      "\n",
      "the the hear in congor thay becom domy, and rid he so withom ile malle; san toul a hat for\n",
      "\n",
      "our the dacthins tho do chis ad ha dorisht of ith toby cenoutind aasend\n",
      "\n",
      "of on the so ore mep ca the culd the bulating wour shis this\n",
      "\n",
      "goverd abesenk of bexort, eloud the this se id len thalve ve surerne tre\n",
      "\n",
      "sende ang wis wof and bas ond tho the sellffgr tove a word has wheze, and\n",
      "\n",
      "tham sore gof beais andy ul and batte in ith sthe mif op\n",
      "---------------\n",
      "\n",
      "\n",
      "The primumened he sorss gad, to the and anding and the we hidip alitt bin of of\n",
      "\n",
      "the deal seanld sind allll, ee to asef sthat pexowered tse whour jo\n",
      "\n",
      "poded f hiners ay e ind ucke to o ble the keet dof the wish sor thage gom oil\n",
      "\n",
      "of and nesche fowith heage as peloiount, I wa she sand cand tors thay\n",
      "\n",
      " hemurstabedin to the bey ind itwited of the ge theat hor ande read so wom enored the\n",
      "\n",
      "him of aserr rimaghte che this the ther dimeang of act an d the\n",
      "\n",
      "tore iptin the they hers tort anten to ve sa lo\n",
      "---------------\n",
      "\n",
      "\n",
      "ad was alangt tho be pont of is.”\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“CHist san inid diss to mate beninie, and if whas end fit all thel the\n",
      "\n",
      "he cive und the in has man id cof ghit lest a hapere, sory and tham\n",
      "\n",
      "thes the a doread you he the roused and the operebing the\n",
      "\n",
      "de ordeinem that o heams fourgh I a the ay sas leandl pead vetelll\n",
      "\n",
      "wall by therne to ano Alld se he to he the he kere the of shey\n",
      "\n",
      "hem the wald she meris s ancuch an howe knot and con ond sad live thous\n",
      "\n",
      "shar is to thaldes bas hin hat tou malld co\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "hake thesaprke for of lly coonging of he to pet whetin in to alo him high ritusis\n",
      "\n",
      "of san I this had to the a endas nof wite nothchin ton to whaFo, and rerd you to co the wo thesh the the sa scure and\n",
      "\n",
      "dee han his ham de beeal of tHem co nant guenderynd the me bontig a\n",
      "\n",
      "as of cy pane usote case an allde; the sof his see this, for rorst hevel an. The wours beawg rite as to\n",
      "\n",
      "at he scioton thims is thim cof hee seloor, uld andlf me has sto\n",
      "\n",
      "thermsis al lee thand he romas be o thele ne berining ot\n",
      "---------------\n",
      "\n",
      "\n",
      "\n",
      "sand and, alle dand and pent and co the dils the the wit I sad his\n",
      "\n",
      "wen ill pon ist to the mo re hish is of this to the hey wof cours and\n",
      "\n",
      "the bay a thave mear to as they bat thave soreas in the he knot swhe thigh\n",
      "\n",
      "corere it fo that as to fand by our aind sronsh of weangrms and wa the to my\n",
      "\n",
      "shivest a Costhalw on she pre itling whre souce cas etre of willl he chiste\n",
      "\n",
      "\n",
      "hert higanns thint re isurd the unowon unld whe Sechelm of e sor co\n",
      "\n",
      "he, mid the wand a rads see a of hory uprt lof de or deplt\n",
      "---------------\n",
      "\n",
      "ce thaks beer am, and the the herm a ofr bugh e of bretin’s ot to\n",
      "\n",
      "the soune us; ther thee as thar hal meaf bet to han ther heam inte\n",
      "\n",
      "to the thive your fofe rene, cort non ton thate the whe se’s hexche, meang beedf\n",
      "\n",
      "sthat has, wicth prilod heas kis Sanchow an ant the hery is and and ha s\n",
      "\n",
      "mealy, Mands thero am mas ith and was ro wust pelired to ith sas heel prine, in But by pa\n",
      "\n",
      "ohe of of le head his sof mity the quis, the palouendes that po arnd omaned ond yor\n",
      "\n",
      "yove hat ond isp that casering in\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "!python sample.py --out_dir=out-don-small-char --device=cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75225da9-4908-4f9a-a1f6-f49ebdeb888c",
   "metadata": {},
   "source": [
    "Generates samples like this:\n",
    "\n",
    "```\n",
    "Sancho nother with this then of everantan has for five he enver any\n",
    "\n",
    "shal were than as in though they and I knight the sther his a jlage,\n",
    "\n",
    "and mad priled and squiel a hist to in feet she took and and sersse to her of\n",
    "\n",
    "Marest and good was pefor rubt some by than lave from his dintat all\n",
    "\n",
    "pack that he remants to goost ever to him arestiance of it the were to who\n",
    "\n",
    "which mom, worly gane for he sporen gort he was roosion, and be that\n",
    "\n",
    "it thou, so so he kniders what the and him of him dest us on shart\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89c51c-d9ee-4206-879e-c39f05aed2cc",
   "metadata": {},
   "source": [
    "*Not bad for ~3 minutes on a CPU, for a hint of the right character gestalt. If you're willing to wait longer, feel free to tune the hyperparameters, increase the size of the network, the context length (`--block_size`), the length of training, etc.*\n",
    "\n",
    "*Finally, on Apple Silicon Macbooks and with a recent PyTorch version make sure to add `--device=mps` (short for \"Metal Performance Shaders\"); PyTorch then uses the on-chip GPU that can *significantly* accelerate training (2-3X) and allow you to use larger networks. See [Issue 28](https://github.com/karpathy/nanoGPT/issues/28) for more.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6eaff-1ca1-4791-a361-b9cef9b822a0",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "Finetuning or transfer learning is a precious method of achieving better models thanks to pre-trained models. Finetuning GPT models is just as simple as training from scratch! We will now download the Don-Quixote (again) but this time we will define it with tokens (using OpenAI's BPE tokenizer) instead of characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c540c-f048-446b-8cc8-214fffdb102d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python data/don/prepare.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b052b-5c7d-4741-b3d5-217966d5ddf6",
   "metadata": {},
   "source": [
    "Run an example finetuning like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ba8e9-3977-43fd-b98d-a37b663d6010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train.py config/finetune_don.py --compile=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040fa6b-64a7-4964-afd5-3c5c545aeaba",
   "metadata": {},
   "source": [
    "This will load the config parameter overrides in `config/finetune_don.py`. Basically, we initialize from a GPT2 checkpoint with `init_from` and train as normal, except shorter and with a small learning rate. Model architecture is changable to `{'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}`) and can be decreased in size by the `block_size` (context length). The best checkpoint (lowest validation loss) will be in the `out_dir` directory, e.g. in `out-don` by default, per the config file. You can then run the code in `sample.py --out_dir=out-don`:\n",
    "```\n",
    "* * All creatures that enter the world below may so far as want to observe the rules of their own land, and may obey them under the hand of their lord, and may not follow others below.\n",
    "\n",
    "* * *\n",
    "\n",
    "THE PORT COLLIDATES,\n",
    "\n",
    "- * *\n",
    "\n",
    "ON the light, and the light to the dark, and the darkness to the light, and the darkness to the darkness, were the present-day laws of monarchy, whose lordship they approved in their faces and hearts. From this moment on, however, they had no other representation to give than that of their master, who, for all that was said or heard, had reached the height of his power.\n",
    "\n",
    "The king's hand, though at times little more than a finger of his, required no more than a finger of his, and that power was, that of holding his eye, and the other of his, in his own, body.\n",
    "\n",
    "When this was spoken of, it was a simple and noble quibble, and the subject of this was so as to admit of the few who had any forsemination, and the few who had the most to go on.\n",
    "\n",
    "The time did not come for a thought of this, and for a moment the very thought of it seemed to fall to the ground.\n",
    "\n",
    "But that thought did not come to pass; though the king was not speaking of the king, it came to pass that the king, with all his might, and all his cunning, and no other sense, and without any understanding, and without any desire for the utmost of his services, and without any desire to put an end to his own glory, and without any desire to hide his triumph, had found the time to say that this was what he thought on the subject of religion; that it was what he thought, and according as it seemed to him to be as good or better to him than to the other kings, and he was in no sense a king, for it seemed to him he could never have any more power than he had to be; that it was a matter of his will and power; and that it was all a matter of his will, for he was determined that this look and that to which he might have been given to hold it was the best in himself.\n",
    "\n",
    "And so it was that the king, who was all around him, and all around him; and so\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037197e-f7e3-40bc-80fd-e2626cf4dc4a",
   "metadata": {},
   "source": [
    "# Inference and Sampling\n",
    "Use the script `sample.py` to sample either from pre-trained GPT-2 models released by OpenAI, or from a model you trained yourself. For example, here is a way to sample from the largest available `gpt2-xl` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d42305-ff4f-4153-be5c-3067e22ccf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sample.py --out_dir=out-don --start=\"Explain the relationship between Don Quixote and Sancho Panza\" --num_samples=5 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9543a34c-4311-481c-8754-e338afbd46de",
   "metadata": {},
   "source": [
    "If you'd like to sample from a model you trained, use the `--out_dir` to point the code appropriately. You can also prompt the model with some text from a file, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f20da7-fc90-4dbf-b860-8385a4323b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python sample.py --start=FILE:\"prompt/fictional.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b1f055-0b1e-4f76-8311-2dd0df84eda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python sample.py --start=FILE:\"prompt/positive_review.txt\" --out_dir=\"out-don\" --num_samples=1 --max_new_tokens=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600147b-8b88-4de0-bc2c-ee2fb7e781d1",
   "metadata": {},
   "source": [
    "I hope you will enjoy with the GPT as much as I did!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c0355-3e89-4ea0-9976-411dc15d76e5",
   "metadata": {},
   "source": [
    "## Efficiency notes\n",
    "\n",
    "*For simple model benchmarking and profiling, `bench.py` might be useful. It's identical to what happens in the meat of the training loop of `train.py`, but omits much of the other complexities.*\n",
    "\n",
    "*Note that the code by default uses [PyTorch 2.0](https://pytorch.org/get-started/pytorch-2.0/). At the time of writing (Dec 29, 2022) this makes `torch.compile()` available in the nightly release. The improvement from the one line of code is noticeable, e.g. cutting down iteration time from ~250ms / iter to 135ms / iter. Nice work PyTorch team!*\n",
    "\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "*Note that by default this repo uses PyTorch 2.0 (i.e. `torch.compile`). This is fairly new and experimental, and not yet available on all platforms (e.g. Windows). If you're running into related error messages try to disable this by adding `--compile=False` flag. This will slow down the code but at least it will run.*\n",
    "\n",
    "*For some context on this repository, GPT, and language modeling it might be helpful to watch [Zero To Hero series](https://karpathy.ai/zero-to-hero.html). Specifically, the [GPT video](https://www.youtube.com/watch?v=kCc8FmEb1nY) is popular if you have some prior language modeling context.*\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This code is a fork from Andrej Karpathy's introductory [NanoGPT repository](https://github.com/karpathy/nanoGPT), which is an updated form of minGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d4e757-8a74-40bf-a36c-9b5f59135121",
   "metadata": {},
   "source": [
    "# Further Experiments\n",
    "\n",
    "(Optional)\n",
    "\n",
    "For further experiments, you can, for example, reproduce the GPT-2, which is still powerful, by following the link to the Andrej Karpathy's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ce869-04d3-4278-a449-a0c8edb1807b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
